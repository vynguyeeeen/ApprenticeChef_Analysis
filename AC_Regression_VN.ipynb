{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd                                   # data science essentials\n",
    "import matplotlib.pyplot as plt                       # data visualization\n",
    "import seaborn as sns                                 # enhanced data visualization\n",
    "import numpy as np                                    # contruct arrays\n",
    "import statsmodels.formula.api as smf                 # statsmodel: OLS model \n",
    "from sklearn.model_selection import train_test_split  # scikit-learn: train/test split \n",
    "from sklearn.linear_model import LinearRegression     # scikit-learn: linear models \n",
    "\n",
    "# set pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# specify the path and file name\n",
    "file = './datasets/Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# read the file into Python\n",
    "df = pd.read_excel(io=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# 1. View number of rows and columns in dataset \n",
    "df.shape\n",
    "\n",
    "#format and print output (0 = rows, 1 = columns)\n",
    "print(f\"\"\"\n",
    "Size of Original Dataset\n",
    "Observations: {df.shape[0]}      \n",
    "Features:     {df.shape[1]}      \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. Check info of each variable (focusing on data type for grouping)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGINEER FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# HOW EACH VARIABLE WAS ENGINEERED: \n",
    "\n",
    "#OBJECTS\n",
    "NAME                         \n",
    "EMAIL                     #split into groups (personal, professional, junk)   \n",
    "FIRST_NAME                 \n",
    "FAMILY_NAME               \n",
    "\n",
    "#CONTINUOUS \n",
    "REVENUE                   #y-variable (Log)\n",
    "TOTAL_MEALS_ORDERED       #Log                 \n",
    "AVG_TIME_PER_SITE_VISIT   #Log \n",
    "AVG_PREP_VID_TIME         #Log \n",
    "TOTAL_PHOTOS_VIEWED       #flag \n",
    "\n",
    "#INTERVAL/COUNT \n",
    "CROSS_SELL_SUCCESS          #0-1 (no engineer)\n",
    "MOBILE_NUMBER               #0-1 (no engineer)\n",
    "TASTES_AND_PREFERENCES      #0-1 (no engineer) \n",
    "WEEKLY_PLAN                 #flag \n",
    "UNIQUE_MEALS_PURCH          #Log     \n",
    "CONTACTS_W_CUSTOMER_SERVICE #Log\n",
    "LARGEST_ORDER_SIZE          #Log\n",
    "AVG_CLICKS_PER_VISIT        #Log\n",
    "PC_LOGINS                   #Log \n",
    "PRODUCT_CATEGORIES_VIEWED   #Log \n",
    "MOBILE_LOGINS               #(no engineer) \n",
    "CANCELLATIONS_BEFORE_NOON   #flag --> Total Cancellations\n",
    "CANCELLATIONS_AFTER_NOON    #flag --> Total Cancellations\n",
    "EARLY_DELIVERIES            #flag --> Total Deliveries\n",
    "LATE_DELIVERIES             #flag --> Total Deliveries \n",
    "REFRIGERATED_LOCKER         #Total Locker\n",
    "PACKAGE_LOCKER              #Total Locker\n",
    "\n",
    "#CATEGORICAL\n",
    "MEDIAN_MEAL_RATING #Label 1-2-3-4-5 and get dummy \n",
    "MASTER_CLASSES_ATTENDED #Label 0-1-2-3 and get dummy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Transform Y-Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: develop a histogram for REVENUE\n",
    "sns.displot(data   = df,\n",
    "                x  = df.loc[:,'REVENUE'],\n",
    "                height = 5,\n",
    "                aspect = 2)\n",
    "\n",
    "# display plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 2: transform REVENUE and save it to the dataset\n",
    "df['log_REVENUE'] = np.log10(df['REVENUE'])\n",
    "\n",
    "# Step 3: plot to check for normality (no more skewness)\n",
    "sns.displot(data = df,\n",
    "            x = 'log_REVENUE',\n",
    "            height = 5,\n",
    "            aspect = 2)\n",
    "\n",
    "# display plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag Trend-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Count variables with the number 0 \n",
    "total_meals_zeroes          = len(df['TOTAL_MEALS_ORDERED']\n",
    "                                  [df['TOTAL_MEALS_ORDERED'] == 0])\n",
    "cancel_before_noon_zeroes   = len(df['CANCELLATIONS_BEFORE_NOON']\n",
    "                                  [df['CANCELLATIONS_BEFORE_NOON'] == 0]) \n",
    "cancel_after_noon_zeroes    = len(df['CANCELLATIONS_AFTER_NOON']\n",
    "                                  [df['CANCELLATIONS_AFTER_NOON'] == 0]) \n",
    "weekly_plan_zeroes          = len(df['WEEKLY_PLAN']\n",
    "                                  [df['WEEKLY_PLAN'] == 0]) \n",
    "early_deliver_zeroes        = len(df['EARLY_DELIVERIES']\n",
    "                                  [df['EARLY_DELIVERIES'] == 0]) \n",
    "late_deliver_zeroes         = len(df['LATE_DELIVERIES']\n",
    "                                  [df['LATE_DELIVERIES'] == 0]) \n",
    "total_photos_zeroes         = len(df['TOTAL_PHOTOS_VIEWED']\n",
    "                                  [df['TOTAL_PHOTOS_VIEWED'] == 0]) \n",
    "\n",
    "# Step 2: Print a table of the results\n",
    "print(f\"\"\"\n",
    "                       No\\t\\tYes\n",
    "                     ---------------------\n",
    "Total Meals         | {total_meals_zeroes}\\t\\t\\t{len(df) - total_meals_zeroes}\n",
    "Cancel Before Noon  | {cancel_before_noon_zeroes}\\t\\t{len(df) - cancel_before_noon_zeroes}\n",
    "Cancel After Noon   | {cancel_after_noon_zeroes}\\t\\t{len(df) - cancel_after_noon_zeroes}\n",
    "Weekly Plan         | {weekly_plan_zeroes}\\t\\t{len(df) - weekly_plan_zeroes}\n",
    "Early Delivery      | {early_deliver_zeroes}\\t\\t{len(df) - early_deliver_zeroes}\n",
    "Late Delivery       | {late_deliver_zeroes}\\t\\t{len(df) - late_deliver_zeroes}\n",
    "Total Photos Viewed | {total_photos_zeroes}\\t\\t{len(df) - total_photos_zeroes}\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 3: flag variables with at least 100 obs for 'yes' and 'no' columns\n",
    "df['HAS_CANCELLATIONS_BEFORE_NOON'] = 0\n",
    "df['HAS_CANCELLATIONS_AFTER_NOON']  = 0\n",
    "df['HAS_WEEKLY_PLAN']               = 0\n",
    "df['HAS_EARLY_DELIVERIES']          = 0\n",
    "df['HAS_LATE_DELIVERIES']           = 0\n",
    "df['HAS_TOTAL_PHOTOS_VIEWED']       = 0\n",
    "\n",
    "# iterate over each original column to change values in the new columns:\n",
    "for index, value in df.iterrows():   \n",
    "    \n",
    "    # CANCELLATIONS_BEFORE_NOON\n",
    "    if df.loc[index, 'CANCELLATIONS_BEFORE_NOON'] > 0:\n",
    "        df.loc[index, 'HAS_CANCELLATIONS_BEFORE_NOON'] = 1\n",
    "        \n",
    "    # CANCELLATIONS_AFTER_NOON\n",
    "    if df.loc[index, 'CANCELLATIONS_AFTER_NOON'] > 0:\n",
    "        df.loc[index, 'HAS_CANCELLATIONS_AFTER_NOON'] = 1\n",
    "        \n",
    "    # WEEKLY_PLAN\n",
    "    if df.loc[index, 'WEEKLY_PLAN'] > 0:\n",
    "        df.loc[index, 'HAS_WEEKLY_PLAN'] = 1 \n",
    "        \n",
    "    # EARLY_DELIVERIES\n",
    "    if df.loc[index, 'EARLY_DELIVERIES'] > 0:\n",
    "        df.loc[index, 'HAS_EARLY_DELIVERIES'] = 1   \n",
    "        \n",
    "    # LATE_DELIVERIES\n",
    "    if df.loc[index, 'LATE_DELIVERIES'] > 0:\n",
    "        df.loc[index, 'HAS_LATE_DELIVERIES'] = 1\n",
    "        \n",
    "    # TOTAL_PHOTOS_VIEWED\n",
    "    if df.loc[index, 'TOTAL_PHOTOS_VIEWED'] > 0:\n",
    "        df.loc[index, 'HAS_TOTAL_PHOTOS_VIEWED'] = 1   \n",
    "        \n",
    "# check results\n",
    "df[['HAS_CANCELLATIONS_BEFORE_NOON', 'HAS_CANCELLATIONS_AFTER_NOON', \n",
    "    'HAS_WEEKLY_PLAN', 'HAS_EARLY_DELIVERIES', 'HAS_LATE_DELIVERIES',\n",
    "    'HAS_TOTAL_PHOTOS_VIEWED']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 4: check the correlations of new features with REVENUE\n",
    "\n",
    "# develop a small correlation matrix\n",
    "zeroes_corr = df.corr()\n",
    "\n",
    "# check the correlations of the newly-created variables with REVENUE\n",
    "zeroes_corr.loc['REVENUE',                                   \n",
    "               ['HAS_CANCELLATIONS_BEFORE_NOON', 'HAS_CANCELLATIONS_AFTER_NOON',\n",
    "                'HAS_WEEKLY_PLAN', 'HAS_EARLY_DELIVERIES', 'HAS_LATE_DELIVERIES',\n",
    "                'HAS_TOTAL_PHOTOS_VIEWED']] \\\n",
    "                .sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform X-Variables Using Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: transform original data using log10 \n",
    "# Step 2: check correlations with Y-variables \n",
    "# Step 3: present results to see improvement in transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# TOTAL_MEALS_ORDERED \n",
    "\n",
    "df['log_TOTAL_MEALS_ORDERED'] = np.log10(df['TOTAL_MEALS_ORDERED'])\n",
    "\n",
    "log_corr = df.loc[ : , ['TOTAL_MEALS_ORDERED',\n",
    "                        'log_TOTAL_MEALS_ORDERED',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['TOTAL_MEALS_ORDERED', 'log_TOTAL_MEALS_ORDERED'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# AVG_TIME_PER_SITE_VISIT\n",
    "\n",
    "df['log_AVG_TIME_PER_SITE_VISIT'] = np.log10(df['AVG_TIME_PER_SITE_VISIT'])\n",
    "\n",
    "log_corr = df.loc[ : , ['AVG_TIME_PER_SITE_VISIT',\n",
    "                        'log_AVG_TIME_PER_SITE_VISIT',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['AVG_TIME_PER_SITE_VISIT', 'log_AVG_TIME_PER_SITE_VISIT'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# AVG_PREP_VID_TIME\n",
    "\n",
    "df['log_AVG_PREP_VID_TIME'] = np.log10(df['AVG_PREP_VID_TIME'])\n",
    "\n",
    "log_corr = df.loc[ : , ['AVG_PREP_VID_TIME',\n",
    "                        'log_AVG_PREP_VID_TIME',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['AVG_PREP_VID_TIME', 'log_AVG_PREP_VID_TIME'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# UNIQUE_MEALS_PURCH      \n",
    "\n",
    "df['log_UNIQUE_MEALS_PURCH'] = np.log10(df['UNIQUE_MEALS_PURCH'])\n",
    "\n",
    "log_corr = df.loc[ : , ['UNIQUE_MEALS_PURCH',\n",
    "                        'log_UNIQUE_MEALS_PURCH',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['UNIQUE_MEALS_PURCH', 'log_UNIQUE_MEALS_PURCH'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# LARGEST_ORDER_SIZE   \n",
    "\n",
    "df['log_LARGEST_ORDER_SIZE'] = np.log10(df['LARGEST_ORDER_SIZE'])\n",
    "\n",
    "log_corr = df.loc[ : , ['LARGEST_ORDER_SIZE',\n",
    "                        'log_LARGEST_ORDER_SIZE',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['LARGEST_ORDER_SIZE', 'log_LARGEST_ORDER_SIZE'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# AVG_CLICKS_PER_VISIT \n",
    "\n",
    "df['log_AVG_CLICKS_PER_VISIT'] = np.log10(df['AVG_CLICKS_PER_VISIT'])\n",
    "\n",
    "log_corr = df.loc[ : , ['AVG_CLICKS_PER_VISIT',\n",
    "                        'log_AVG_CLICKS_PER_VISIT',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['AVG_CLICKS_PER_VISIT', 'log_AVG_CLICKS_PER_VISIT'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# PC_LOGINS        \n",
    "\n",
    "df['log_PC_LOGINS'] = np.log10(df['PC_LOGINS'])\n",
    "\n",
    "log_corr = df.loc[ : , ['PC_LOGINS',\n",
    "                        'log_PC_LOGINS',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['PC_LOGINS', 'log_PC_LOGINS'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# PRODUCT_CATEGORIES_VIEWED\n",
    "\n",
    "df['log_PRODUCT_CATEGORIES_VIEWED'] = np.log10(df['PRODUCT_CATEGORIES_VIEWED'])\n",
    "\n",
    "log_corr = df.loc[ : , ['PRODUCT_CATEGORIES_VIEWED',\n",
    "                        'log_PRODUCT_CATEGORIES_VIEWED',\n",
    "                        'REVENUE',\n",
    "                        'log_REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                                 .round(decimals = 2)\n",
    "\n",
    "log_corr.loc[ ['PRODUCT_CATEGORIES_VIEWED', 'log_PRODUCT_CATEGORIES_VIEWED'],\n",
    "              ['REVENUE', 'log_REVENUE']   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Interval/Count Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# TOTAL DELIVERIES \n",
    "\n",
    "# Step 1: combine features \n",
    "df['TOTAL_DELIVERY'] = df['EARLY_DELIVERIES'] + df['LATE_DELIVERIES']\n",
    "\n",
    "\n",
    "# Step 2: develop a correlation matrix \n",
    "delivery_corr = df.loc[ : , ['EARLY_DELIVERIES',\n",
    "                              'LATE_DELIVERIES',\n",
    "                              'TOTAL_DELIVERY',\n",
    "                              'REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                              .round(decimals = 2)\n",
    "\n",
    "# Step 3: print value counts and correlations deliveries\n",
    "print(f\"\"\"\n",
    "---------------------\n",
    "Correlations\n",
    "---------------------\n",
    "{delivery_corr['REVENUE']}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# TOTAL LOCKERS \n",
    "\n",
    "# Step 1: combine features \n",
    "df['TOTAL_LOCKERS'] = df['REFRIGERATED_LOCKER'] + df['PACKAGE_LOCKER']\n",
    "\n",
    "# Step 2: develop a correlation matrix \n",
    "locker_corr = df.loc[ : , ['REFRIGERATED_LOCKER',\n",
    "                              'PACKAGE_LOCKER',\n",
    "                              'TOTAL_LOCKERS',\n",
    "                              'REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                              .round(decimals = 2)\n",
    "\n",
    "# Step 3: print value counts and correlations deliveries\n",
    "print(f\"\"\"\n",
    "---------------------\n",
    "Correlations\n",
    "---------------------\n",
    "{locker_corr['REVENUE']}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# TOTAL CANCELLATION \n",
    "\n",
    "# Step 1: combine features \n",
    "df['TOTAL_CANCELLATION'] = df['CANCELLATIONS_BEFORE_NOON'] + \\\n",
    "                            df['CANCELLATIONS_AFTER_NOON']\n",
    "\n",
    "\n",
    "# Step 2: develop a correlation matrix \n",
    "cancel_corr = df.loc[ : , ['CANCELLATIONS_BEFORE_NOON',\n",
    "                              'CANCELLATIONS_AFTER_NOON',\n",
    "                              'TOTAL_CANCELLATION',\n",
    "                              'REVENUE']  ].corr(method = 'pearson')\\\n",
    "                                              .round(decimals = 2)\n",
    "\n",
    "# Step 3: print value counts and correlations deliveries\n",
    "print(f\"\"\"\n",
    "---------------------\n",
    "Correlations\n",
    "---------------------\n",
    "{cancel_corr['REVENUE']}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Labels and One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# MASTER_CLASSES_ATTENDED\n",
    "\n",
    "# Step 1: assign labels to data \n",
    "df['MASTER_CLASS_ATTENDED_TIERS'] = pd.cut(df['MASTER_CLASSES_ATTENDED'], bins=[-1,0,1,2,3],\\\n",
    "                           labels = [\"Never\",\"Rarely\", \"Occasionally\", \"Regularly\"])\n",
    "\n",
    "# Step 2: one hot encode variable\n",
    "one_hot_attendance = pd.get_dummies(df['MASTER_CLASS_ATTENDED_TIERS'])\n",
    "\n",
    "# Step 3: drop variable after they've been encoded\n",
    "df = df.drop('MASTER_CLASS_ATTENDED_TIERS', axis = 1)\n",
    "\n",
    "# Step 4: join coding together\n",
    "df = df.join([one_hot_attendance])\n",
    "\n",
    "# Step 5: save new columns\n",
    "new_columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# MEDIAN_MEAL_RATING\n",
    "\n",
    "# Step 1: assign labels to data \n",
    "df['MEDIAN_MEAL_RATING_TIERS'] = pd.cut(df['MEDIAN_MEAL_RATING'], bins=[-1,0,1,2,3,4,5],\\\n",
    "                        labels = [\"Terrible\",\"Bad\",\"Decent\", \"Good\", \"Great\", \"Excellent\"])\n",
    "\n",
    "# Step 2: one hot encode variable\n",
    "one_hot_tiers    = pd.get_dummies(df['MEDIAN_MEAL_RATING_TIERS'])\n",
    "\n",
    "# Step 3: drop variable after they've been encoded\n",
    "df = df.drop('MEDIAN_MEAL_RATING_TIERS', axis = 1)\n",
    "\n",
    "# Step 4: join coding together\n",
    "df = df.join([one_hot_tiers])\n",
    "\n",
    "# Step 5: save new columns\n",
    "new_columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split EMAILS into Domains and One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 1: split EMAILS\n",
    "\n",
    "# create an empty placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# loop over each email address\n",
    "for index, col in df.iterrows():\n",
    "    \n",
    "    # split email domain at '@'\n",
    "    split_email = df.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # append placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "# convert placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 2: concatenate with original DataFrame\n",
    "\n",
    "# rename column to concatenate\n",
    "email_df.columns = ['account_name' , 'domain'] \n",
    "\n",
    "# concatenate 'domain' to  original DataFrame \n",
    "df = pd.concat([df, email_df['domain']],   \n",
    "                   axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 3: aggregate emails into groups\n",
    "\n",
    "# specify domain types \n",
    "professional = [    '@mmm.com',\n",
    "                    '@amex.com',\n",
    "                    '@apple.com',\n",
    "                    '@boeing.com',\n",
    "                    '@caterpillar.com',\n",
    "                    '@chevron.com', \n",
    "                    '@cisco.com', \n",
    "                    '@cocacola.com', \n",
    "                    '@disney.com', \n",
    "                    '@dupont.com', \n",
    "                    '@exxon.com', \n",
    "                    '@ge.org',\n",
    "                    '@goldmansacs.com',\n",
    "                    '@homedepot.com', \n",
    "                    '@ibm.com', \n",
    "                    '@intel.com', \n",
    "                    '@jnj.com',\n",
    "                    '@jpmorgan.com',\n",
    "                    '@mcdonalds.com', \n",
    "                    '@merck.com', \n",
    "                    '@microsoft.com',\n",
    "                    '@nike.com', \n",
    "                    '@pfizer.com', \n",
    "                    '@pg.com', \n",
    "                    '@travelers.com',\n",
    "                    '@unitedtech.com',\n",
    "                    '@unitedhealth.com', \n",
    "                    '@verizon.com', \n",
    "                    '@visa.com', \n",
    "                    '@walmart.com'       ]\n",
    "\n",
    "personal   = [      '@gmail.com', \n",
    "                    '@yahoo.com', \n",
    "                    '@protonmail.com'    ]\n",
    "\n",
    "junk       = [      '@me.com',\n",
    "                    '@aol.com',\n",
    "                    '@hotmail.com', \n",
    "                    '@live.com', \n",
    "                    '@msn.com',\n",
    "                    '@passport.com'      ]\n",
    "\n",
    "# create an empty placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# loop to group observations by domain type\n",
    "for domain in df['domain']:\n",
    "        if '@' + domain in personal:\n",
    "            placeholder_lst.append('professional')\n",
    "                  \n",
    "        elif '@' + domain in junk:\n",
    "            placeholder_lst.append('personal')\n",
    "        \n",
    "        elif '@' + domain in professional:\n",
    "            placeholder_lst.append('junk')\n",
    "            \n",
    "        else:\n",
    "            print('Unknown')\n",
    "\n",
    "# concatenate with original DataFrame\n",
    "df['group_domain'] = pd.Series(placeholder_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 4: get dummy variables for 'domain'\n",
    "\n",
    "# one hot encode variable \n",
    "one_hot_domain    = pd.get_dummies(df['group_domain'])\n",
    "\n",
    "# drop variables after they've been encoded\n",
    "df = df.drop('group_domain', axis = 1)\n",
    "\n",
    "# join coding together\n",
    "df = df.join([one_hot_domain])\n",
    "\n",
    "# save new columns\n",
    "new_columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORDINARY LEAST SQUARES (OLS) MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using full dataset with original Y-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Prepare data for train/split test \n",
    "# prepare explanatory variables by dropping objects and y-variables\n",
    "df_data   = df.drop(['REVENUE',\n",
    "                    'log_REVENUE',\n",
    "                    'NAME', \n",
    "                    'EMAIL', \n",
    "                    'FIRST_NAME', \n",
    "                    'FAMILY_NAME',\n",
    "                    'domain'], axis = 1)\n",
    "\n",
    "# prepare response variables\n",
    "df_target =df.loc[ : , 'REVENUE']\n",
    "log_df_target =df.loc[ : , 'log_REVENUE']\n",
    "\n",
    "# Step 2: set up train-test split \n",
    "x_train_FULL, x_test_FULL, y_train_FULL, y_test_FULL = train_test_split(\n",
    "            df_data,     # x-variables\n",
    "            df_target,   # original y-variable\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)\n",
    "\n",
    "# Step 3: double check the shapes of the dataset\n",
    "print(f\"\"\"\n",
    "Training Data\n",
    "-------------\n",
    "X-side: {x_train_FULL.shape}\n",
    "y-side: {y_train_FULL.shape}\n",
    "\n",
    "\n",
    "Testing Data\n",
    "------------\n",
    "X-side: {x_test_FULL.shape}\n",
    "y-side: {y_test_FULL.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 4: conduct test\n",
    "\n",
    "# instantiate a model object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# fit to the training data\n",
    "lr_fit = lr.fit(x_train_FULL, y_train_FULL)\n",
    "\n",
    "# predict on new data\n",
    "lr_pred = lr_fit.predict(x_test_FULL)\n",
    "\n",
    "# score the results\n",
    "print('OLS Training Score :', lr.score(x_train_FULL, y_train_FULL).round(4))  \n",
    "print('OLS Testing Score  :',  lr.score(x_test_FULL, y_test_FULL).round(4)) \n",
    "\n",
    "# save scores for future use \n",
    "lr_train_score = lr.score(x_train_FULL, y_train_FULL).round(4) \n",
    "lr_test_score  = lr.score(x_test_FULL, y_test_FULL).round(4)   \n",
    "\n",
    "# display and save the gap between training and testing\n",
    "print('OLS Train-Test Gap :', abs(lr_train_score - lr_test_score).round(4))\n",
    "lr_test_gap = abs(lr_train_score - lr_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using full dataset with transformed Y-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Prepare data for train/split test \n",
    "# prepare explanatory variables by dropping objects and y-variables\n",
    "df_data   = df.drop(['REVENUE',\n",
    "                    'log_REVENUE',\n",
    "                    'NAME', \n",
    "                    'EMAIL', \n",
    "                    'FIRST_NAME', \n",
    "                    'FAMILY_NAME',\n",
    "                    'domain'], axis = 1)\n",
    "\n",
    "# prepare response variables\n",
    "df_target =df.loc[ : , 'REVENUE']\n",
    "log_df_target =df.loc[ : , 'log_REVENUE']\n",
    "\n",
    "# Step 2: set up train-test split \n",
    "x_train_log, x_test_log, y_train_log, y_test_log = train_test_split(\n",
    "            df_data,         # x-variables\n",
    "            log_df_target,   # transformed y-variable\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)\n",
    "\n",
    "# Step 3: double check the shapes of the dataset\n",
    "print(f\"\"\"\n",
    "Training Data\n",
    "-------------\n",
    "X-side: {x_train_log.shape}\n",
    "y-side: {y_train_log.shape}\n",
    "\n",
    "\n",
    "Testing Data\n",
    "------------\n",
    "X-side: {x_test_log.shape}\n",
    "y-side: {y_test_log.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 4: conduct test\n",
    "\n",
    "# instantiate a model object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# fit to the training data\n",
    "lr_fit = lr.fit(x_train_log, y_train_log)\n",
    "\n",
    "# predict on new data\n",
    "lr_pred = lr_fit.predict(x_test_log)\n",
    "\n",
    "# score the results\n",
    "print('OLS Training Score :', lr.score(x_train_log, y_train_log).round(4)) \n",
    "print('OLS Testing Score  :',  lr.score(x_test_log, y_test_log).round(4))\n",
    "\n",
    "# save scores for future use \n",
    "lr_train_score = lr.score(x_train_log, y_train_log).round(4)\n",
    "lr_test_score  = lr.score(x_test_log, y_test_log).round(4)  \n",
    "\n",
    "# display and save the gap between training and testing\n",
    "print('OLS Train-Test Gap :', abs(lr_train_score - lr_test_score).round(4))\n",
    "lr_test_gap = abs(lr_train_score - lr_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find model efficients and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# zip each feature name to its coefficient\n",
    "lr_model_values = zip(df_data.columns,\n",
    "                      lr_fit.coef_.round(decimals = 2))\n",
    "\n",
    "# set up a placeholder list to store model features\n",
    "lr_model_lst = [('intercept', lr_fit.intercept_.round(decimals = 2))]\n",
    "\n",
    "# print out each feature-coefficient pair one by one\n",
    "for val in lr_model_values:\n",
    "    lr_model_lst.append(val)\n",
    "    \n",
    "# check the results\n",
    "for pair in lr_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary of OLS model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Type'    : ['OLS'],\n",
    "           \n",
    "    'Training' : [lr_train_score],\n",
    "           \n",
    "    'Testing'  : [lr_test_score],\n",
    "                    \n",
    "    'Train-Test Gap' : [lr_test_gap],\n",
    "                    \n",
    "    'Model Size' : [len(lr_model_lst)],\n",
    "                    \n",
    "    'Model' : [lr_model_lst]}\n",
    "\n",
    "# convert model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "# send model results to Excel\n",
    "model_performance.to_excel('./model_results/assignment_linear_model_performance.xlsx',\n",
    "                           index = False)\n",
    "\n",
    "# read final model output\n",
    "pd.read_excel('./model_results/assignment_linear_model_performance.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

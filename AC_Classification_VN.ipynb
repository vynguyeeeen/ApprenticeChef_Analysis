{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# importing packages\n",
    "########################################\n",
    "\n",
    "# all libraries\n",
    "import pandas as pd                                     # data science essentials\n",
    "import matplotlib.pyplot as plt                         # data visualization\n",
    "import seaborn as sns                                   # enhanced data visualization\n",
    "import numpy as np                                      # contruct arrays\n",
    "\n",
    "# classficiation packages\n",
    "import statsmodels.formula.api as smf                   # logistic regression\n",
    "from sklearn.linear_model import LogisticRegression     # logistic regression\n",
    "from sklearn.model_selection import train_test_split    # train-test split\n",
    "from sklearn.tree import DecisionTreeClassifier         # decision trees\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest  \n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm \n",
    "\n",
    "# tuning packages  \n",
    "from sklearn.model_selection import RandomizedSearchCV  # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                 # customizable scorer\n",
    "\n",
    "# evaluation metrics packages \n",
    "from sklearn.metrics import confusion_matrix            # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score               # auc score\n",
    "\n",
    "\n",
    "########################################\n",
    "# setting display options and loading data\n",
    "########################################\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# specify the path and file name\n",
    "file = './datasets/Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# read the file into Python\n",
    "df = pd.read_excel(io=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def text_split_feature(col, df, sep=' ', new_col_name='NUMBER_OF_NAMES'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGINEER FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flag Trend-based Features:**   \n",
    "Flagging features containing many zeroes and creating dummy variables for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# make new columns and assign 0 as placeholder\n",
    "df['HAS_CANCELLATIONS_BEFORE_NOON'] = 0\n",
    "df['HAS_CANCELLATIONS_AFTER_NOON']  = 0\n",
    "df['HAS_WEEKLY_PLAN']               = 0\n",
    "df['HAS_EARLY_DELIVERIES']          = 0\n",
    "df['HAS_LATE_DELIVERIES']           = 0\n",
    "df['HAS_TOTAL_PHOTOS_VIEWED']       = 0\n",
    "df['HAS_MASTER_CLASSES_ATTENDED']   = 0\n",
    "\n",
    "# iterate over each original column to change values in the new columns:\n",
    "for index, value in df.iterrows():   \n",
    "    \n",
    "    # HAS_CANCELLATIONS_BEFORE_NOON\n",
    "    if df.loc[index, 'CANCELLATIONS_BEFORE_NOON'] > 0:\n",
    "        df.loc[index, 'HAS_CANCELLATIONS_BEFORE_NOON'] = 1\n",
    "        \n",
    "    # HAS_CANCELLATIONS_AFTER_NOON\n",
    "    if df.loc[index, 'CANCELLATIONS_AFTER_NOON'] > 0:\n",
    "        df.loc[index, 'HAS_CANCELLATIONS_AFTER_NOON'] = 1\n",
    "        \n",
    "    # HAS_WEEKLY_PLAN\n",
    "    if df.loc[index, 'WEEKLY_PLAN'] > 0:\n",
    "        df.loc[index, 'HAS_WEEKLY_PLAN'] = 1 \n",
    "        \n",
    "    # HAS_EARLY_DELIVERIES\n",
    "    if df.loc[index, 'EARLY_DELIVERIES'] > 0:\n",
    "        df.loc[index, 'HAS_EARLY_DELIVERIES'] = 1   \n",
    "        \n",
    "    # HAS_LATE_DELIVERIES\n",
    "    if df.loc[index, 'LATE_DELIVERIES'] > 0:\n",
    "        df.loc[index, 'HAS_LATE_DELIVERIES'] = 1\n",
    "        \n",
    "    # HAS_TOTAL_PHOTOS_VIEWED\n",
    "    if df.loc[index, 'TOTAL_PHOTOS_VIEWED'] > 0:\n",
    "        df.loc[index, 'HAS_TOTAL_PHOTOS_VIEWED'] = 1   \n",
    "    \n",
    "    # HAS_MASTER_CLASSES_ATTENDED\n",
    "    if df.loc[index, 'MASTER_CLASSES_ATTENDED'] > 0:\n",
    "        df.loc[index, 'HAS_MASTER_CLASSES_ATTENDED'] = 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Features:**   \n",
    "Splitting feature 'EMAIL' to show username and domain, and create dummy variable for domain groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 1: split EMAIL\n",
    "\n",
    "# create an empty placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# loop over each email address\n",
    "for index, col in df.iterrows():\n",
    "    \n",
    "    # split email domain at '@'\n",
    "    split_email = df.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # append placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "# convert placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 2: concatenate with original DataFrame\n",
    "\n",
    "# rename column to concatenate\n",
    "email_df.columns = ['account_name' , 'domain'] \n",
    "\n",
    "# concatenate 'domain' to  original DataFrame \n",
    "df = pd.concat([df, email_df['domain']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 3: aggregate emails into groups\n",
    "\n",
    "# specify domain types \n",
    "professional = [    '@mmm.com',\n",
    "                    '@amex.com',\n",
    "                    '@apple.com',\n",
    "                    '@boeing.com',\n",
    "                    '@caterpillar.com',\n",
    "                    '@chevron.com', \n",
    "                    '@cisco.com', \n",
    "                    '@cocacola.com', \n",
    "                    '@disney.com', \n",
    "                    '@dupont.com', \n",
    "                    '@exxon.com', \n",
    "                    '@ge.org',\n",
    "                    '@goldmansacs.com',\n",
    "                    '@homedepot.com', \n",
    "                    '@ibm.com', \n",
    "                    '@intel.com', \n",
    "                    '@jnj.com',\n",
    "                    '@jpmorgan.com',\n",
    "                    '@mcdonalds.com', \n",
    "                    '@merck.com', \n",
    "                    '@microsoft.com',\n",
    "                    '@nike.com', \n",
    "                    '@pfizer.com', \n",
    "                    '@pg.com', \n",
    "                    '@travelers.com',\n",
    "                    '@unitedtech.com',\n",
    "                    '@unitedhealth.com', \n",
    "                    '@verizon.com', \n",
    "                    '@visa.com', \n",
    "                    '@walmart.com'       ]\n",
    "\n",
    "personal   = [      '@gmail.com', \n",
    "                    '@yahoo.com', \n",
    "                    '@protonmail.com'    ]\n",
    "\n",
    "junk       = [      '@me.com',\n",
    "                    '@aol.com',\n",
    "                    '@hotmail.com', \n",
    "                    '@live.com', \n",
    "                    '@msn.com',\n",
    "                    '@passport.com'      ]\n",
    "\n",
    "# create an empty placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# loop to group observations by domain type\n",
    "for domain in df['domain']:\n",
    "        if '@' + domain in personal:\n",
    "            placeholder_lst.append('personal')\n",
    "                  \n",
    "        elif '@' + domain in junk:\n",
    "            placeholder_lst.append('junk')\n",
    "        \n",
    "        elif '@' + domain in professional:\n",
    "            placeholder_lst.append('professional')\n",
    "            \n",
    "        else:\n",
    "            print('Unknown')\n",
    "\n",
    "# concatenate with original DataFrame\n",
    "df['group_domain'] = pd.Series(placeholder_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Step 4: get dummy variables for 'domain'\n",
    "\n",
    "# one hot encode variable \n",
    "one_hot_domain    = pd.get_dummies(df['group_domain'])\n",
    "\n",
    "# join coding together\n",
    "df = df.join([one_hot_domain])\n",
    "\n",
    "# save new columns\n",
    "new_columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Develop New Features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Count names and assign values to new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# New column: NUMBER_OF_NAMES\n",
    "\n",
    "# split names using text_split_feature\n",
    "text_split_feature('NAME', df, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Combine features to make new ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# creating a column for average cost per meal\n",
    "df['AVG_PRICE_PER_ORDER'] = df['REVENUE']/df['TOTAL_MEALS_ORDERED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the median to set thresholds:\n",
    "df['WEEKLY_PLAN'].median()\n",
    "\n",
    "# repeat step above for following features:\n",
    "# professional\n",
    "# UNIQUE_MEALS_PURCH\n",
    "# TASTES_AND_PREFERENCES\n",
    "# MEDIAN_MEAL_RATING\n",
    "# AVG_PRICE_PER_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# make new columns with 0 as placeholder \n",
    "df['WEEKLY_WORKING']    = 0   # weekly plans using professional emails\n",
    "df['UNIQUE_TASTE_PREF'] = 0   # made unique purchases based on prerferences \n",
    "df['MEDIAN_RATER']      = 0   # median rating based on cost per meal \n",
    "\n",
    "\n",
    "# iterate over each original column to change values in the new columns:\n",
    "for index, value in df.iterrows(): \n",
    "\n",
    "    # WEEKLY_WORKING \n",
    "    if df.loc[index, 'WEEKLY_PLAN'] >= 7 and \\\n",
    "    df.loc[index, 'professional'] == 1:\n",
    "        df.loc[index, 'WEEKLY_WORKING'] = 1      \n",
    "    \n",
    "    # UNIQUE_TASTE_PREF\n",
    "    if df.loc[index, 'UNIQUE_MEALS_PURCH'] > 5 and \\\n",
    "    df.loc[index, 'TASTES_AND_PREFERENCES'] == 1:\n",
    "        df.loc[index, 'UNIQUE_TASTE_PREF'] = 1 \n",
    "    \n",
    "    # MEDIAN_RATER\n",
    "    if df.loc[index, 'MEDIAN_MEAL_RATING'] <= 3 and \\\n",
    "    df.loc[index, 'AVG_PRICE_PER_ORDER'] > 34:\n",
    "        df.loc[index, 'MEDIAN_RATER'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE FOR MODELING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Unnecessary Features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# drop variables after they've been encoded\n",
    "df = df.drop('NAME', axis = 1)\n",
    "df = df.drop('EMAIL', axis = 1)\n",
    "df = df.drop('FIRST_NAME', axis = 1)\n",
    "df = df.drop('FAMILY_NAME', axis =1)\n",
    "df = df.drop('domain', axis = 1)\n",
    "df = df.drop('group_domain', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# check correlation with all x-variables\n",
    "df_corr = df.corr(method = 'pearson').round(2)\n",
    "\n",
    "df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and Split Data for Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# making a copy of the dataset\n",
    "df_data = pd.DataFrame.copy(df) \n",
    "\n",
    "# declare explanatory variables - drop y-variable only\n",
    "df_data = df.drop(['CROSS_SELL_SUCCESS'], axis = 1)\n",
    "\n",
    "# declare response variable\n",
    "df_target =  df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_data,\n",
    "                                                    df_target,\n",
    "                                                    random_state = 219,\n",
    "                                                    test_size    = 0.25,\n",
    "                                                    stratify     = df_target)\n",
    "\n",
    "# merge training data for statsmodels\n",
    "df_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Develop a Logistic Regression with Summary Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " REVENUE + \n",
      " TOTAL_MEALS_ORDERED + \n",
      " UNIQUE_MEALS_PURCH + \n",
      " CONTACTS_W_CUSTOMER_SERVICE + \n",
      " PRODUCT_CATEGORIES_VIEWED + \n",
      " AVG_TIME_PER_SITE_VISIT + \n",
      " MOBILE_NUMBER + \n",
      " CANCELLATIONS_BEFORE_NOON + \n",
      " CANCELLATIONS_AFTER_NOON + \n",
      " TASTES_AND_PREFERENCES + \n",
      " PC_LOGINS + \n",
      " MOBILE_LOGINS + \n",
      " WEEKLY_PLAN + \n",
      " EARLY_DELIVERIES + \n",
      " LATE_DELIVERIES + \n",
      " PACKAGE_LOCKER + \n",
      " REFRIGERATED_LOCKER + \n",
      " AVG_PREP_VID_TIME + \n",
      " LARGEST_ORDER_SIZE + \n",
      " MASTER_CLASSES_ATTENDED + \n",
      " MEDIAN_MEAL_RATING + \n",
      " AVG_CLICKS_PER_VISIT + \n",
      " TOTAL_PHOTOS_VIEWED + \n",
      " HAS_CANCELLATIONS_BEFORE_NOON + \n",
      " HAS_CANCELLATIONS_AFTER_NOON + \n",
      " HAS_WEEKLY_PLAN + \n",
      " HAS_EARLY_DELIVERIES + \n",
      " HAS_LATE_DELIVERIES + \n",
      " HAS_TOTAL_PHOTOS_VIEWED + \n",
      " HAS_MASTER_CLASSES_ATTENDED + \n",
      " junk + \n",
      " personal + \n",
      " professional + \n",
      " NUMBER_OF_NAMES + \n",
      " AVG_PRICE_PER_ORDER + \n",
      " WEEKLY_WORKING + \n",
      " UNIQUE_TASTE_PREF + \n",
      " MEDIAN_RATER + \n"
     ]
    }
   ],
   "source": [
    "# print all features in dataset \n",
    "for val in df_data:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.542122\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>CROSS_SELL_SUCCESS</td> <th>  No. Observations:  </th>  <td>  1459</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>Logit</td>       <th>  Df Residuals:      </th>  <td>  1449</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>        <th>  Df Model:          </th>  <td>     9</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 25 Jan 2021</td>  <th>  Pseudo R-squ.:     </th>  <td>0.1367</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:46:31</td>      <th>  Log-Likelihood:    </th> <td> -790.96</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>              <td>True</td>        <th>  LL-Null:           </th> <td> -916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>  LLR p-value:       </th> <td>7.923e-49</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                   <td>   -1.7062</td> <td>    0.324</td> <td>   -5.270</td> <td> 0.000</td> <td>   -2.341</td> <td>   -1.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>junk</th>                        <td>   -1.3095</td> <td>    0.157</td> <td>   -8.339</td> <td> 0.000</td> <td>   -1.617</td> <td>   -1.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>professional</th>                <td>    0.5960</td> <td>    0.144</td> <td>    4.129</td> <td> 0.000</td> <td>    0.313</td> <td>    0.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NUMBER_OF_NAMES</th>             <td>    0.5567</td> <td>    0.094</td> <td>    5.894</td> <td> 0.000</td> <td>    0.372</td> <td>    0.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_BEFORE_NOON</th>   <td>    0.2791</td> <td>    0.046</td> <td>    6.007</td> <td> 0.000</td> <td>    0.188</td> <td>    0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_NUMBER</th>               <td>    0.9185</td> <td>    0.178</td> <td>    5.162</td> <td> 0.000</td> <td>    0.570</td> <td>    1.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REFRIGERATED_LOCKER</th>         <td>    0.5069</td> <td>    0.209</td> <td>    2.430</td> <td> 0.015</td> <td>    0.098</td> <td>    0.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CONTACTS_W_CUSTOMER_SERVICE</th> <td>    0.0658</td> <td>    0.028</td> <td>    2.367</td> <td> 0.018</td> <td>    0.011</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MEDIAN_RATER</th>                <td>   -0.4282</td> <td>    0.126</td> <td>   -3.390</td> <td> 0.001</td> <td>   -0.676</td> <td>   -0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_TASTE_PREF</th>           <td>    0.3282</td> <td>    0.138</td> <td>    2.384</td> <td> 0.017</td> <td>    0.058</td> <td>    0.598</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:     CROSS_SELL_SUCCESS   No. Observations:                 1459\n",
       "Model:                          Logit   Df Residuals:                     1449\n",
       "Method:                           MLE   Df Model:                            9\n",
       "Date:                Mon, 25 Jan 2021   Pseudo R-squ.:                  0.1367\n",
       "Time:                        23:46:31   Log-Likelihood:                -790.96\n",
       "converged:                       True   LL-Null:                       -916.19\n",
       "Covariance Type:            nonrobust   LLR p-value:                 7.923e-49\n",
       "===============================================================================================\n",
       "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "Intercept                      -1.7062      0.324     -5.270      0.000      -2.341      -1.072\n",
       "junk                           -1.3095      0.157     -8.339      0.000      -1.617      -1.002\n",
       "professional                    0.5960      0.144      4.129      0.000       0.313       0.879\n",
       "NUMBER_OF_NAMES                 0.5567      0.094      5.894      0.000       0.372       0.742\n",
       "CANCELLATIONS_BEFORE_NOON       0.2791      0.046      6.007      0.000       0.188       0.370\n",
       "MOBILE_NUMBER                   0.9185      0.178      5.162      0.000       0.570       1.267\n",
       "REFRIGERATED_LOCKER             0.5069      0.209      2.430      0.015       0.098       0.916\n",
       "CONTACTS_W_CUSTOMER_SERVICE     0.0658      0.028      2.367      0.018       0.011       0.120\n",
       "MEDIAN_RATER                   -0.4282      0.126     -3.390      0.001      -0.676      -0.181\n",
       "UNIQUE_TASTE_PREF               0.3282      0.138      2.384      0.017       0.058       0.598\n",
       "===============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a logreg using significant variables only (p-value < 0.05)\n",
    "logit_sig = smf.logit(formula = \"\"\"  CROSS_SELL_SUCCESS ~\n",
    "                                        junk + \n",
    "                                        professional + \n",
    "                                        NUMBER_OF_NAMES + \n",
    "                                        CANCELLATIONS_BEFORE_NOON + \n",
    "                                        MOBILE_NUMBER + \n",
    "                                        REFRIGERATED_LOCKER +\n",
    "                                        CONTACTS_W_CUSTOMER_SERVICE + \n",
    "                                        MEDIAN_RATER + \n",
    "                                        UNIQUE_TASTE_PREF\"\"\",  \n",
    "                                        data    = df_train)\n",
    "\n",
    "# fitting the model object\n",
    "results_sig = logit_sig.fit()\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_sig.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Develop a dictionary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary to store candidate models with significant variables \n",
    "\n",
    "candidate_dict = {\n",
    " \n",
    " # significant variables only\n",
    " 'logit_sig' : ['professional','junk','NUMBER_OF_NAMES',\n",
    "                'CANCELLATIONS_BEFORE_NOON','MOBILE_NUMBER', 'MOBILE_LOGINS',\n",
    "                'REFRIGERATED_LOCKER','CONTACTS_W_CUSTOMER_SERVICE',\n",
    "                'MEDIAN_RATER', 'UNIQUE_TASTE_PREF',\n",
    "                'CANCELLATIONS_AFTER_NOON'],\n",
    " \n",
    " 'logit_sig2' : [ 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', \n",
    "                  'TASTES_AND_PREFERENCES', 'PC_LOGINS', 'EARLY_DELIVERIES',\n",
    "                  'REFRIGERATED_LOCKER', 'junk', 'professional', \n",
    "                  'NUMBER_OF_NAMES', 'CONTACTS_W_CUSTOMER_SERVICE']\n",
    "    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY CLASSIFICATION MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "df_data   =  df.loc[ : , candidate_dict['logit_sig']]\n",
    "df_target =  df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                                                    df_data,\n",
    "                                                    df_target,\n",
    "                                                    random_state = 219,\n",
    "                                                    test_size    = 0.25,\n",
    "                                                    stratify     = df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-8f5bc172d26d>:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  C_space          = pd.np.arange(0.1, 5.0, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'warm_start': True, 'solver': 'newton-cg', 'C': 3.9000000000000004}\n",
      "Tuned CV AUC      : 0.6528\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# find best hyperparameters for tuning\n",
    "########################################\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "C_space          = pd.np.arange(0.1, 5.0, 0.1)\n",
    "warm_start_space = [True, False]\n",
    "solver_space     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'C'          : C_space,\n",
    "              'warm_start' : warm_start_space,\n",
    "              'solver'     : solver_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "lr_tuned = LogisticRegression(random_state = 219,\n",
    "                              max_iter     = 1000)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "                                 param_distributions = param_grid, # parameters to tune\n",
    "                                 cv                  = 3,          # how many folds in cross-validation\n",
    "                                 n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "                                 random_state        = 219,        # starting point for random sequence\n",
    "                                 scoring = make_scorer(\n",
    "                                           roc_auc_score,\n",
    "                                           needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "lr_tuned_cv.fit(df_data, df_target)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3.9000000000000004, max_iter=1000, random_state=219,\n",
       "                   solver='newton-cg', warm_start=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tuned_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7382\n",
      "Testing  ACCURACY: 0.7515\n",
      "AUC Score        : 0.6664\n"
     ]
    }
   ],
   "source": [
    "# instantiate a logistic regression model with tuned values\n",
    "lr_tuned = LogisticRegression(C=3.9000000000000004, \n",
    "                              max_iter=1000, random_state=219,\n",
    "                              solver='newton-cg', warm_start=True)\n",
    "\n",
    "# fit to training set\n",
    "lr_tuned_fit = lr_tuned.fit(x_train, y_train)\n",
    "\n",
    "# predict based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(x_test)\n",
    "\n",
    "\n",
    "# score the results\n",
    "print('Training ACCURACY:', lr_tuned.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', lr_tuned.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# save scoring data for future use\n",
    "lr_tuned_train_score = lr_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "lr_tuned_test_score  = lr_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# save the AUC score\n",
    "lr_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = lr_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 67\n",
      "False Positives: 89\n",
      "False Negatives: 32\n",
      "True Positives : 299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "lr_tuned_tn, \\\n",
    "lr_tuned_fp, \\\n",
    "lr_tuned_fn, \\\n",
    "lr_tuned_tp = confusion_matrix(y_true = y_test, y_pred = lr_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {lr_tuned_tn}\n",
    "False Positives: {lr_tuned_fp}\n",
    "False Negatives: {lr_tuned_fn}\n",
    "True Positives : {lr_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Logistic Regression</td>\n",
       "      <td>0.6664</td>\n",
       "      <td>0.7382</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>(67, 89, 32, 299)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Name  AUC Score  Training Accuracy  Testing Accuracy   Confusion Matrix\n",
       "0  Tuned Logistic Regression     0.6664             0.7382            0.7515  (67, 89, 32, 299)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declaring model performance objects\n",
    "lr_train_acc = lr_tuned.score(x_train, y_train).round(4)\n",
    "lr_test_acc  = lr_tuned.score(x_test, y_test).round(4)\n",
    "lr_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = lr_tuned_pred).round(4)\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'        : ['Tuned Logistic Regression'],\n",
    "           \n",
    "    'AUC Score'         : [lr_tuned_auc],\n",
    "    \n",
    "    'Training Accuracy' : [lr_tuned_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [lr_tuned_test_score],\n",
    "\n",
    "    'Confusion Matrix'  : [(lr_tuned_tn, lr_tuned_fp, lr_tuned_fn, lr_tuned_tp)]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "model_performance.to_excel('model_performance.xlsx',\n",
    "                           index = False)\n",
    "\n",
    "\n",
    "# checking for results\n",
    "pd.read_excel('model_performance.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the FULL dataset (all x-variables)\n",
    "df_data   =  df.drop([ 'CROSS_SELL_SUCCESS'], axis = 1)\n",
    "df_target =  df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    " \n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                                                    df_data,\n",
    "                                                    df_target,\n",
    "                                                    random_state = 219,\n",
    "                                                    test_size    = 0.25,\n",
    "                                                    stratify     = df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'min_samples_leaf': 1, 'max_depth': 3, 'criterion': 'entropy'}\n",
      "Tuned CV AUC      : 0.7492\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# find best hyperparameters for tuning\n",
    "########################################\n",
    "\n",
    "# set the parameters and distributions to sample\n",
    "param_grid = {\"max_depth\": [3, 8],\n",
    "              \"min_samples_leaf\": [1,25],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_tuned_cv = RandomizedSearchCV(tree, param_grid, \n",
    "                             cv=7,                    \n",
    "                             random_state = 219)       \n",
    "\n",
    "# Fit it to the data\n",
    "tree_tuned_cv.fit(df_data, df_target)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tree_tuned_cv.best_params_)\n",
    "print(\"Tuned CV AUC      :\", tree_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=3)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_tuned_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7402\n",
      "Testing  ACCURACY: 0.7762\n",
      "AUC Score        : 0.732\n"
     ]
    }
   ],
   "source": [
    "# instantiate a decision tree with tuned values\n",
    "tree_tuned = DecisionTreeClassifier(criterion='gini', \n",
    "                                    max_depth=3,\n",
    "                                    min_samples_leaf = 25,\n",
    "                                    random_state=219)\n",
    "\n",
    "\n",
    "# fit to the training set\n",
    "tree_tuned_fit = tree_tuned.fit(x_train, y_train)\n",
    "\n",
    "# predict based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# score the results\n",
    "print('Training ACCURACY:', tree_tuned.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# save scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# save the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                       y_score = tree_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 95\n",
      "False Positives: 61\n",
      "False Negatives: 48\n",
      "True Positives : 283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "tree_tuned_tn, \\\n",
    "tree_tuned_fp, \\\n",
    "tree_tuned_fn, \\\n",
    "tree_tuned_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tree_tuned_tn}\n",
    "False Positives: {tree_tuned_fp}\n",
    "False Negatives: {tree_tuned_fn}\n",
    "True Positives : {tree_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tree_train_acc = tree_tuned.score(x_train, y_train).round(4)\n",
    "tree_test_acc  = tree_tuned.score(x_test, y_test).round(4)\n",
    "tree_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = tree_tuned_pred).round(4)\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'             : 'Tuned Tree (Full)',\n",
    "                          'Training Accuracy'  : tree_train_acc,\n",
    "                          'Testing Accuracy'   : tree_test_acc,\n",
    "                          'AUC Score'          : tree_auc,\n",
    "                          'Confusion Matrix'   : (tree_tuned_tn,\n",
    "                                                  tree_tuned_fp,\n",
    "                                                  tree_tuned_fn,\n",
    "                                                  tree_tuned_tp)},\n",
    "                          ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "df_data   =  df.loc[ : , candidate_dict['logit_sig']]\n",
    "df_target =  df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                                                    df_data,\n",
    "                                                    df_target,\n",
    "                                                    random_state = 219,\n",
    "                                                    test_size    = 0.25,\n",
    "                                                    stratify     = df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-152-c9c3eeb542e1>:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  estimator_space  = pd.np.arange(100, 1100, 250)\n",
      "<ipython-input-152-c9c3eeb542e1>:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  leaf_space       = pd.np.arange(1, 31, 10)\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 96 is smaller than n_iter=500. Running 96 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'warm_start': True, 'n_estimators': 100, 'min_samples_leaf': 11, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Tuned Training AUC: 0.6874\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# find best hyperparameters for tuning\n",
    "########################################\n",
    "\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "leaf_space       = pd.np.arange(1, 31, 10)\n",
    "criterion_space  = ['gini', 'entropy']\n",
    "bootstrap_space  = [True, False]\n",
    "warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_space,\n",
    "              'min_samples_leaf' : leaf_space,\n",
    "              'criterion'        : criterion_space,\n",
    "              'bootstrap'        : bootstrap_space,\n",
    "              'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "                               param_distributions = param_grid,\n",
    "                               cv                  = 5,\n",
    "                               n_iter              = 500,\n",
    "                               scoring             = make_scorer(roc_auc_score,\n",
    "                                                     needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "forest_cv.fit(df_data, df_target)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", forest_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, criterion='entropy',\n",
       "                       min_samples_leaf=11, random_state=219, warm_start=True)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Tuned Training ACCURACY: 0.7814\n",
      "Forest Tuned Testing  ACCURACY: 0.7782\n",
      "Forest Tuned AUC Score        : 0.6979\n"
     ]
    }
   ],
   "source": [
    "# building a model based on tuning results\n",
    "forest_tuned = RandomForestClassifier(criterion='entropy',\n",
    "                                      bootstrap=False, \n",
    "                                      min_samples_leaf=11, \n",
    "                                      random_state=219, \n",
    "                                      warm_start=True)\n",
    "\n",
    "# FITTING the model object\n",
    "forest_tuned_fit = forest_tuned.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(x_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(x_test, y_test).round(4))\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                       y_score = forest_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 74\n",
      "False Positives: 82\n",
      "False Negatives: 26\n",
      "True Positives : 305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_rf_tn, \\\n",
    "tuned_rf_fp, \\\n",
    "tuned_rf_fn, \\\n",
    "tuned_rf_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_rf_tn}\n",
    "False Positives: {tuned_rf_fp}\n",
    "False Negatives: {tuned_rf_fn}\n",
    "True Positives : {tuned_rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tuned_rf_train_acc = forest_tuned_fit.score(x_train, y_train).round(4)\n",
    "tuned_rf_test_acc  = forest_tuned_fit.score(x_test, y_test).round(4)\n",
    "tuned_rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = forest_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Tuned Random Forest',\n",
    "                           'Training Accuracy'  : tuned_rf_train_acc,\n",
    "                           'Testing Accuracy'   : tuned_rf_test_acc,\n",
    "                           'AUC Score'          : tuned_rf_auc,\n",
    "                           'Confusion Matrix'   : (tuned_rf_tn,\n",
    "                                                   tuned_rf_fp,\n",
    "                                                   tuned_rf_fn,\n",
    "                                                   tuned_rf_tp)},\n",
    "                          ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# train/test split with FULL dataset (all x-variables)\n",
    "df_data   =  df.drop([ 'CROSS_SELL_SUCCESS'], axis = 1)\n",
    "df_target =  df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    " \n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                                                    df_data,\n",
    "                                                    df_target,\n",
    "                                                    random_state = 219,\n",
    "                                                    test_size    = 0.25,\n",
    "                                                    stratify     = df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7752\n",
      "Testing  ACCURACY: 0.7598\n",
      "AUC Score        : 0.6758\n"
     ]
    }
   ],
   "source": [
    "# building a model with hyperparameter manually tuned \n",
    "gbm_tuned = GradientBoostingClassifier(learning_rate = 0.3,\n",
    "                                        criterion     = 'friedman_mse',\n",
    "                                        min_samples_leaf =1,\n",
    "                                        max_depth     = 1,\n",
    "                                        warm_start    = False,\n",
    "                                        random_state  = 219)\n",
    "\n",
    "# FIT step is needed \n",
    "gbm_tuned_fit = gbm_tuned.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', gbm_tuned_fit.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = gbm_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 69\n",
      "False Positives: 87\n",
      "False Negatives: 30\n",
      "True Positives : 301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tuned_gbm_train_acc = gbm_tuned_fit.score(x_train, y_train).round(4)\n",
    "tuned_gbm_test_acc  = gbm_tuned_fit.score(x_test, y_test).round(4)\n",
    "tuned_gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = gbm_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Tuned GBM (Full)',\n",
    "                           'Training Accuracy'  : tuned_gbm_train_acc,\n",
    "                           'Testing Accuracy'   : tuned_gbm_test_acc,\n",
    "                           'AUC Score'          : tuned_gbm_auc,\n",
    "                           'Confusion Matrix'   : (gbm_tuned_tn,\n",
    "                                                   gbm_tuned_fp,\n",
    "                                                   gbm_tuned_fn,\n",
    "                                                   gbm_tuned_tp)},\n",
    "                          ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Model Scores:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuned Tree (Full)</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>(95, 61, 48, 283)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tuned Random Forest</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.7782</td>\n",
       "      <td>(74, 82, 26, 305)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned GBM (Full)</td>\n",
       "      <td>0.6758</td>\n",
       "      <td>0.7752</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>(69, 87, 30, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Logistic Regression</td>\n",
       "      <td>0.6664</td>\n",
       "      <td>0.7382</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>(67, 89, 32, 299)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Name  AUC Score  Training Accuracy  Testing Accuracy   Confusion Matrix\n",
       "1          Tuned Tree (Full)     0.7320             0.7402            0.7762  (95, 61, 48, 283)\n",
       "2        Tuned Random Forest     0.6979             0.7814            0.7782  (74, 82, 26, 305)\n",
       "3           Tuned GBM (Full)     0.6758             0.7752            0.7598  (69, 87, 30, 301)\n",
       "0  Tuned Logistic Regression     0.6664             0.7382            0.7515  (67, 89, 32, 299)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show model with highest AUC at top \n",
    "model_performance.sort_values(by = 'AUC Score',\n",
    "                              ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# saving results to Excel\n",
    "model_performance.to_excel('./model_results/classification_model_performance.xlsx',\n",
    "                           index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Chosen Model & Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuned Decision Tree is the model with the best AUC score.    \n",
    "The training-testing gap also remains below 0.05. Therefore, the model was not overfitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name           Tuned Tree (Full)\n",
       "AUC Score                        0.732\n",
       "Training Accuracy               0.7402\n",
       "Testing Accuracy                0.7762\n",
       "Confusion Matrix     (95, 61, 48, 283)\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display chosen model only \n",
    "final_model =  model_performance.loc[ 1 , : ]\n",
    "final_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
